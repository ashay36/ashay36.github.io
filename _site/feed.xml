<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en_us"><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en_us" /><updated>2021-10-04T16:28:29+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">Ashay Ajbani</title><author><name>Sal</name></author><entry><title type="html">Autonomously Navigating a Robot using RRT Path Planning algorithm in ROS</title><link href="http://localhost:4000/autonomous-robot-in-ros/" rel="alternate" type="text/html" title="Autonomously Navigating a Robot using RRT Path Planning algorithm in ROS" /><published>2021-05-12T00:00:00+05:30</published><updated>2021-05-12T00:00:00+05:30</updated><id>http://localhost:4000/autonomous-robot-in-ros</id><content type="html" xml:base="http://localhost:4000/autonomous-robot-in-ros/">&lt;p&gt;The project includes a robot being spawned in a restaurant world. The robot travels autonomously around the restaurant using the map developed using SLAM and by control commands generated by move_base package in ROS which uses the RRT algorithm for path plannning.&lt;/p&gt;

&lt;h3 id=&quot;to-run-the-code-in-your-machine-following-are-the-steps--&quot;&gt;To run the code in your machine, following are the steps :-&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Close the repository in your machine.&lt;/li&gt;
  &lt;li&gt;Copy the packages in your workspace.&lt;/li&gt;
  &lt;li&gt;Terminal 1 - catkin_make&lt;/li&gt;
  &lt;li&gt;Terminal 1 - source devel/setup.bash&lt;/li&gt;
  &lt;li&gt;Terminal 1 - roslaunch path_planning simulation.launch&lt;/li&gt;
  &lt;li&gt;Terminal 2 - roslaunch path_planning plan_path.launch&lt;/li&gt;
  &lt;li&gt;Give a goal location using “2D Nav Goal” option in RViz&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;restaurant-world&quot;&gt;Restaurant World&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/world.png&quot; alt=&quot;Restaurant&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;rviz-window&quot;&gt;RViz window&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/rviz.png&quot; alt=&quot;RViz&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;rviz-window-with-planned-path&quot;&gt;RViz window with planned path&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/path_planning.png&quot; alt=&quot;RViz with planned path&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Click &lt;a href=&quot;https://github.com/ashay36/Autonomously-Navigating-a-Robot-using-RRT-Path-Planning-algorithm-in-ROS&quot;&gt;here&lt;/a&gt; to access the code&lt;/p&gt;</content><author><name>Ashay</name></author><category term="Robotics" /><category term="ROS" /><summary type="html">The project includes a robot being spawned in a restaurant world. The robot travels autonomously around the restaurant using the map developed using SLAM and by control commands generated by move_base package in ROS which uses the RRT algorithm for path plannning.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/turtlebot3.jpg" /><media:content medium="image" url="http://localhost:4000/assets/images/turtlebot3.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">RRT (Rapidly Exploring Random Trees) Path Planning Algorithm</title><link href="http://localhost:4000/rrt/" rel="alternate" type="text/html" title="RRT (Rapidly Exploring Random Trees) Path Planning Algorithm" /><published>2021-01-21T00:00:00+05:30</published><updated>2021-01-21T00:00:00+05:30</updated><id>http://localhost:4000/rrt</id><content type="html" xml:base="http://localhost:4000/rrt/">&lt;p&gt;This project includes the implementation of RRT (Rapidly exploring Random Trees) path planning algorithm in Python and C++. The code has been documented for better readability and understanding.
The start, goal and obstacle co-ordinates can be changed from within the code itself.&lt;/p&gt;

&lt;h2 id=&quot;python&quot;&gt;Python&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;RRT Python&lt;/code&gt; folder contains the python file of the algorithm.&lt;/p&gt;

&lt;h4 id=&quot;dependencies&quot;&gt;Dependencies&lt;/h4&gt;
&lt;ul&gt;
    &lt;li&gt;
        &lt;a href=&quot;https://numpy.org/&quot;&gt;NumPy&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
        &lt;a href=&quot;https://matplotlib.org/&quot;&gt;Matplotlib&lt;/a&gt;
    &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;running-code-via-command-line&quot;&gt;Running code via command line&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Change directory to where the code file is&lt;/li&gt;
  &lt;li&gt;Execute command - &lt;code&gt;python rrt_python.py&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;c&quot;&gt;C++&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;RRT C++&lt;/code&gt; folder contains the C++ files of the algorithm. It also contains the Eigen and Matplotlibcpp library files required for the project&lt;/p&gt;

&lt;h4 id=&quot;dependencis&quot;&gt;Dependencis&lt;/h4&gt;
&lt;ul&gt;
    &lt;li&gt;
        &lt;a href=&quot;http://eigen.tuxfamily.org/index.php?title=Main_Page#License&quot;&gt;Eigen&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
        &lt;a href=&quot;https://github.com/lava/matplotlib-cpp&quot;&gt;Matplotlib-cpp&lt;/a&gt;
    &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;running-code-via-termial-ubuntu&quot;&gt;Running code via Termial (Ubuntu)&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Change the directory to where the code files are&lt;/li&gt;
  &lt;li&gt;Execute command - &lt;code&gt;g++ main.cpp -I/path/to/Python.h/file -lpython-version&lt;/code&gt;. 
In my case, the command is - &lt;code&gt;g++ main.cpp -I/usr/include/python3.8 -lpython3.8&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;After successfully executing above command, execute command - &lt;code&gt;./a.out&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Click &lt;a href=&quot;https://github.com/ashay36/RRT-Rapidly-Exploring-Random-Trees-&quot;&gt;here&lt;/a&gt; to access the code&lt;/p&gt;

&lt;p&gt;Check out the RRT research paper &lt;a href=&quot;http://msl.cs.uiuc.edu/~lavalle/papers/LavKuf01.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Ashay</name></author><category term="Robotics" /><category term="Path Planning" /><summary type="html">This project includes the implementation of RRT (Rapidly exploring Random Trees) path planning algorithm in Python and C++. The code has been documented for better readability and understanding. The start, goal and obstacle co-ordinates can be changed from within the code itself.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/rrt.png" /><media:content medium="image" url="http://localhost:4000/assets/images/rrt.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Wheat Detection</title><link href="http://localhost:4000/wheat-detection/" rel="alternate" type="text/html" title="Wheat Detection" /><published>2020-08-23T00:00:00+05:30</published><updated>2020-08-23T00:00:00+05:30</updated><id>http://localhost:4000/wheat-detection</id><content type="html" xml:base="http://localhost:4000/wheat-detection/">&lt;p&gt;The competition was hosted on Kaggle.
The task was to detect and localize wheat heads in images.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Team Name : The Alphas

Authors : Ashay Ajbani and Pritam Rao

Competition Type : Object Detection

Framework : PyTorch

Solution Type : Transfer Learning

Number of Classes : 1

Pretrained Models Used : 
1) Detection Transformer (Submission 1)
2) Efficient Detector (Submission 2)

Public Leaderboard Rank : 481/2270

Private Leaderboard Rank : 354/2270
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;our-approach&quot;&gt;Our Approach&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Submission 1&lt;/strong&gt; :&lt;/p&gt;

&lt;p&gt;For our first submission, we fine tuned pretrained &lt;em&gt;Efficient Detector&lt;/em&gt; model for our problem.
It gave us a score of 0.7331 on the public leaderboard.&lt;/p&gt;

&lt;p&gt;Click &lt;a href=&quot;https://github.com/ashay36/Machine-Learning-Competitions-Notebooks/tree/master/Kaggle%20-%20Global%20Wheat%20Detection/Efficient%20Detector&quot;&gt;here&lt;/a&gt; to access the code&lt;/p&gt;

&lt;p&gt;The file &lt;em&gt;wheat_detection_efficientdet.ipynb&lt;/em&gt; includes training as well as inference. We have used Test Time 
Augmentation and Weighted Boxes Fusion for making more accurate predictions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Submission 2&lt;/strong&gt; :&lt;/p&gt;

&lt;p&gt;For our second submission, we fine tuned pretrained &lt;em&gt;DETR&lt;/em&gt; model for our problem. It gave us a 
score of 0.5758 on the public leaderboard.&lt;/p&gt;

&lt;p&gt;Click &lt;a href=&quot;https://github.com/ashay36/Machine-Learning-Competitions-Notebooks/tree/master/Kaggle%20-%20Global%20Wheat%20Detection/DEtection%20TRansformer%20(DETR)&quot;&gt;here&lt;/a&gt; to access the code&lt;/p&gt;

&lt;p&gt;We have uploaded the following DETR notebooks :&lt;/p&gt;

&lt;p&gt;1) &lt;em&gt;wheat_detection_detr_training.ipynb&lt;/em&gt; : Includes processing the dataset and training on DETR.&lt;/p&gt;

&lt;p&gt;2) &lt;em&gt;wheat_detection_detr_inference.ipynb&lt;/em&gt; : Making predictions on the test images using the trained model.&lt;/p&gt;

&lt;p&gt;We have documented the notebooks so that you can easily reproduce the results.&lt;/p&gt;</content><author><name>Ashay</name></author><category term="Deep Learning" /><category term="Object Detection" /><category term="Competitions" /><summary type="html">The competition was hosted on Kaggle. The task was to detect and localize wheat heads in images.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/wheat.jpg" /><media:content medium="image" url="http://localhost:4000/assets/images/wheat.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Emergency Vehicle Classification</title><link href="http://localhost:4000/vehicle-classification/" rel="alternate" type="text/html" title="Emergency Vehicle Classification" /><published>2020-08-08T00:00:00+05:30</published><updated>2020-08-08T00:00:00+05:30</updated><id>http://localhost:4000/vehicle-classification</id><content type="html" xml:base="http://localhost:4000/vehicle-classification/">&lt;p&gt;The competition was hosted on Analytics Vidhya.
The task was to classify vehicles into emergency and non-emergency categories.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Team Name : The Alphas

Authors : Ashay Ajbani and Pritam Rao

Competition Type : Image Classification

Framework : Keras

Solution Type : Model Ensembling

Number of Classes : 2

Pretrained Models Used : VGG16 and NASnet Large

Private Leaderboard Rank : 36/10000
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;our-approach&quot;&gt;Our Approach&lt;/h3&gt;
&lt;p&gt;We ensembled 5 models to make final predictions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Models&lt;/strong&gt; :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; Model 1 : Fine Tuning VGG16 &lt;/li&gt;
&lt;li&gt; Model 2 : Fine Tuning VGG16 with l2 regularization &lt;/li&gt;
&lt;li&gt; Model 3 : k-fold validation &lt;/li&gt;
&lt;li&gt; Model 4 : Fine Tuning NASnet &lt;/li&gt;
&lt;li&gt; Model 5 : Fine Tuning NASnet with l2 regularization&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The predictions taken by ensembling above 5 models
resulted in an accuracy greater than any of the individual models.&lt;/p&gt;

&lt;p&gt;Techniques used to reduce overfitting :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt; Data Augmentation &lt;/li&gt;
  &lt;li&gt; BatchNormalization &lt;/li&gt;
  &lt;li&gt; Dropout &lt;/li&gt;
  &lt;li&gt; l2 Regularization &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Accuracy&lt;/strong&gt; :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; Model 1 : 0.9498 &lt;/li&gt;
&lt;li&gt; Model 2 : 0.9546 &lt;/li&gt;
&lt;li&gt; Model 3 : 0.9444 &lt;/li&gt;
&lt;li&gt; Model 4 : 0.9548 &lt;/li&gt;
&lt;li&gt; Model 5 : 0.9514 &lt;/li&gt;
&lt;li&gt; After Ensembling (final) : 0.9688 &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Click &lt;a href=&quot;https://github.com/ashay36/Machine-Learning-Competitions-Notebooks/tree/master/Analytics%20Vidhya&apos;s%20Computer%20Vision%20Competition&quot;&gt;here&lt;/a&gt; to access the code&lt;/p&gt;</content><author><name>Ashay</name></author><category term="Deep Learning" /><category term="Image Classification" /><category term="Competitions" /><summary type="html">The competition was hosted on Analytics Vidhya. The task was to classify vehicles into emergency and non-emergency categories.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/vehicle.jpg" /><media:content medium="image" url="http://localhost:4000/assets/images/vehicle.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Face Applications</title><link href="http://localhost:4000/face-applications/" rel="alternate" type="text/html" title="Face Applications" /><published>2020-06-17T00:00:00+05:30</published><updated>2020-06-17T00:00:00+05:30</updated><id>http://localhost:4000/face-applications</id><content type="html" xml:base="http://localhost:4000/face-applications/">&lt;p&gt;This project contains face related applications using opencv and deep learning libraries.&lt;/p&gt;

&lt;h4 id=&quot;face-recognition-using-lbph-face-recognizer&quot;&gt;Face Recognition using LBPH Face Recognizer&lt;/h4&gt;

&lt;p&gt;The pipeline involves the following steps :-&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt; Detecting faces using OpenCV&apos;s Haar Cascades &lt;/li&gt;
    &lt;li&gt; Training a Face Recognizer using OpenCV&apos;s build-in LBPH Face Recognizer &lt;/li&gt;
    &lt;li&gt; Saving the model and predicting on custom images &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Click &lt;a href=&quot;https://github.com/ashay36/Face-Applications/tree/master/Face%20Recognition%20using%20LBPHFaceRecognizer&quot;&gt;here&lt;/a&gt; to access the code.&lt;/p&gt;

&lt;h4 id=&quot;face-recognition-using-one-shot-learning&quot;&gt;Face Recognition using One Shot Learning&lt;/h4&gt;

&lt;p&gt;On a contrary to the above Face Recognition method which involves training on several images, One Shot learning only requires a pre-trained to directly get the embeddings from the detected face. The code involves the following steps :-&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt; Detecting faces using MTCNN Face Detector &lt;/li&gt;
    &lt;li&gt; Getting feature embeddings of each class from pre-trained VGGFace model and storing them in a database &lt;/li&gt;
    &lt;li&gt; Get the feature embeddings of the test image and calculate the distance between this test image and each embedding in the database &lt;/li&gt;
    &lt;li&gt; If the distance is less than a threshold for a particular pair, then the identity has been found &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Click &lt;a href=&quot;https://github.com/ashay36/Face-Applications/tree/master/Face%20Recognition%20-%20One%20Shot%20Learning&quot;&gt;here&lt;/a&gt; to access the code&lt;/p&gt;

&lt;h4 id=&quot;face-alignment&quot;&gt;Face Alignment&lt;/h4&gt;

&lt;p&gt;It is one of the most important steps in modern day Face Recognition pipeline to increase the accuracy of recognition.&lt;/p&gt;

&lt;p&gt;Click &lt;a href=&quot;https://github.com/ashay36/Face-Applications/tree/master/Face%20Alignment&quot;&gt;here&lt;/a&gt; to access the code.&lt;/p&gt;</content><author><name>Ashay</name></author><category term="Computer Vision" /><category term="Deep Learning" /><category term="Projects" /><summary type="html">This project contains face related applications using opencv and deep learning libraries.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/face_applications.jpg" /><media:content medium="image" url="http://localhost:4000/assets/images/face_applications.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">COVID-19 Face Mask Detection</title><link href="http://localhost:4000/face-mask-detection/" rel="alternate" type="text/html" title="COVID-19 Face Mask Detection" /><published>2020-05-21T00:00:00+05:30</published><updated>2020-05-21T00:00:00+05:30</updated><id>http://localhost:4000/face-mask-detection</id><content type="html" xml:base="http://localhost:4000/face-mask-detection/">&lt;p&gt;A deep learning based model that detects whether a person is wearing a face mask or not with nearly 100% accuracy.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Framework : Keras

Pretrained model used : MobileNetV2

Trainable parameters : 3M

Number of classes : 2

Train Accuracy : 99.29%

Validation Accuracy : 100%
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;dependencies&quot;&gt;Dependencies&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt; &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;Tensorflow&lt;/a&gt; &lt;/li&gt;
  &lt;li&gt; &lt;a href=&quot;https://www.keras.io/&quot;&gt;Keras&lt;/a&gt; &lt;/li&gt;
  &lt;li&gt; &lt;a href=&quot;https://www.opencv.org/&quot;&gt;OpenCV&lt;/a&gt; &lt;/li&gt;
  &lt;li&gt; &lt;a href=&quot;https://www.numpy.org/&quot;&gt;NumPy&lt;/a&gt; &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;&lt;b&gt; Techniques used to reduce overfitting :- &lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt; Data Augmentation &lt;/li&gt;
  &lt;li&gt; BatchNormalization &lt;/li&gt;
  &lt;li&gt; Dropout &lt;/li&gt;
  &lt;li&gt; Early Stopping &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Click &lt;a href=&quot;https://github.com/ashay36/COVID-19-Face-Mask-Detection-&quot;&gt;here&lt;/a&gt; to checkout the code.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; To run the code :- &lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For training refer &lt;i&gt;&lt;b&gt;mask_detection.ipynb&lt;/b&gt;&lt;/i&gt;&lt;/li&gt;
  &lt;li&gt;For testing on your own image run &lt;i&gt;&lt;b&gt;detect_facemask.py&lt;/b&gt;&lt;/i&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Ashay</name></author><category term="Deep Learning" /><category term="Image Classification" /><category term="Projects" /><summary type="html">A deep learning based model that detects whether a person is wearing a face mask or not with nearly 100% accuracy.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/face_mask.jpg" /><media:content medium="image" url="http://localhost:4000/assets/images/face_mask.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Fruits Classifier</title><link href="http://localhost:4000/fruits-classifier/" rel="alternate" type="text/html" title="Fruits Classifier" /><published>2020-05-16T00:00:00+05:30</published><updated>2020-05-16T00:00:00+05:30</updated><id>http://localhost:4000/fruits-classifier</id><content type="html" xml:base="http://localhost:4000/fruits-classifier/">&lt;p&gt;A classifier that can classify up-to 120 different types of fruits and vegetables with 95% accuracy&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Framework : Keras

Pretrained model used : VGG19

Trainable parameters : 8.37M

Number of classes : 120

Train accuracy: 97.39%

Validation accuracy : 95.64%
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I have used VGG19 model pre-trained on the Imagenet dataset and fine-tuned it using Transfer Learning
technique.&lt;/p&gt;

&lt;p&gt;Many different combinations of hyperparameters were experimented of which this model
proved to be more promising. The notebook also includes the code to unzip the dataset.&lt;/p&gt;

&lt;p&gt;Techniques used to reduce overfitting :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt; Data Augmentation &lt;/li&gt;
  &lt;li&gt; BatchNormalization &lt;/li&gt;
  &lt;li&gt; Dropout &lt;/li&gt;
  &lt;li&gt; Early Stopping &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Click &lt;a href=&quot;https://github.com/ashay36/Fruits-Classifier/blob/master/fruits360_model.ipynb&quot;&gt;here&lt;/a&gt; to access the code.&lt;/p&gt;

&lt;p&gt;Click &lt;a href=&quot;https://www.kaggle.com/moltean/fruits&quot;&gt;here&lt;/a&gt; to download the dataset.&lt;/p&gt;</content><author><name>Ashay</name></author><category term="Deep Learning" /><category term="Image Classification" /><category term="Projects" /><summary type="html">A classifier that can classify up-to 120 different types of fruits and vegetables with 95% accuracy</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/fruits.jpg" /><media:content medium="image" url="http://localhost:4000/assets/images/fruits.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">YOLO Object Detection</title><link href="http://localhost:4000/yolo-object-detection/" rel="alternate" type="text/html" title="YOLO Object Detection" /><published>2020-05-16T00:00:00+05:30</published><updated>2020-05-16T00:00:00+05:30</updated><id>http://localhost:4000/yolo-object-detection</id><content type="html" xml:base="http://localhost:4000/yolo-object-detection/">&lt;p&gt;The code implements Object Detection using YOLOv3. The code has been implemented using the OpenCV library.
It uses a YOLOv3 model pre-trained on the COCO dataset.&lt;/p&gt;

&lt;h3 id=&quot;dependencies&quot;&gt;Dependencies&lt;/h3&gt;

&lt;ul&gt;
    &lt;li&gt;
        &lt;a href=&quot;https://opencv.org/&quot;&gt;OpenCV&lt;/a&gt;
    &lt;/li&gt;
    &lt;li&gt;
        &lt;a href=&quot;https://numpy.org/&quot;&gt;NumPy&lt;/a&gt;
    &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The weights can be downloaded from &lt;a href=&quot;https://pjreddie.com/media/files/yolov3.weights&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;input&quot;&gt;Input&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/input.jpg&quot; alt=&quot;Dog&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;output&quot;&gt;Output&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/test.jpg&quot; alt=&quot;Output&quot; /&gt;&lt;/p&gt;

&lt;p&gt;No. of objects detected - 3 &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Click &lt;a href=&quot;https://github.com/ashay36/YOLO-Object-Detection-using-OpenCV&quot;&gt;here&lt;/a&gt; to access the code.&lt;/p&gt;</content><author><name>Ashay</name></author><category term="Deep Learning" /><category term="Object Detection" /><category term="Projects" /><summary type="html">The code implements Object Detection using YOLOv3. The code has been implemented using the OpenCV library. It uses a YOLOv3 model pre-trained on the COCO dataset.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/yolo.png" /><media:content medium="image" url="http://localhost:4000/assets/images/yolo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>